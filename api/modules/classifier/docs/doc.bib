% Encoding: UTF-8
@misc{PBSProAdminGuide14,
	title 	= {PBS Professional\textregistered 14.1 Administrator's Guide},
	author = {Altair PBS Works},
	year = {2016}
}


@article{emani2015understandable,
  title={Understandable big data: A survey},
  author={Emani, Cheikh Kacfah and Cullot, Nadine and Nicolle, Christophe},
  journal={Computer science review},
  volume={17},
  pages={70--81},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{agerwala_keynote:_2009,
	title = {Keynote: {Challenges} on the road to exascale computing},
	shorttitle = {Keynote},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-92990-1_1},
	urldate = {2016-09-07},
	booktitle = {International {Conference} on {High}-{Performance} {Embedded} {Architectures} and {Compilers}},
	publisher = {Springer},
	author = {Agerwala, Tilak},
	year = {2009},
	note = {00001},
	pages = {1--1},
	file = {Agerwala - 2009 - Keynote Challenges on the road to exascale comput.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\37W4UN8W\\Agerwala - 2009 - Keynote Challenges on the road to exascale comput.pdf:application/pdf}
}


@inproceedings{shalf2010exascale,
  title={Exascale computing technology challenges},
  author={Shalf, John and Dosanjh, Sudip and Morrison, John},
  booktitle={International Conference on High Performance Computing for Computational Science},
  pages={1--25},
  year={2010},
  organization={Springer}
}



@article{al_international_2011,
	title = {The {International} {Exascale} {Software} {Project} roadmap},
	issn = {1094-3420, 1741-2846},
	url = {http://hpc.sagepub.com/content/early/2011/01/04/1094342010391989},
	doi = {10.1177/1094342010391989},
	abstract = {Over the last 20 years, the open-source community has provided more and more software on which the world’s high-performance computing systems depend for performance and productivity. The community has invested millions of dollars and years of effort to build key components. However, although the investments in these separate software elements have been tremendously valuable, a great deal of productivity has also been lost because of the lack of planning, coordination, and key integration of technologies necessary to make them work together smoothly and efficiently, both within individual petascale systems and between different systems. It seems clear that this completely uncoordinated development model will not provide the software needed to support the unprecedented parallelism required for peta/exascale computation on millions of cores, or the flexibility required to exploit new hardware models and features, such as transactional memory, speculative execution, and graphics processing units. This report describes the work of the community to prepare for the challenges of exascale computing, ultimately combing their efforts in a coordinated International Exascale Software Project.},
	language = {en},
	urldate = {2016-09-07},
	journal = {Int. J. High Perform. Comput. Appl.},
	author = {Jack Dongarra and others},
	month = jan,
	year = {2011},
	keywords = {challenges, exascale systems, trends},
	file = {Al_2011_The International Exascale Software Project roadmap.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\UEHG96H6\\Al_2011_The International Exascale Software Project roadmap.pdf:application/pdf;Snapshot:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\KDNCVUVK\\1094342010391989.html:text/html}
}

@TechReport{ashby_opportunities_2010,
  author =      {Ashby, Steve and others},
  title =       {The {Opportunities} and {Challenges} of {Exascale} {Computing}: {Summary} {Report} of the {Advanced} {Scientific} {Computing} {Advisory} {Committee} ({ASCAC}) {Subcommittee}},
  institution = {({ASCAC})},
  year =        {2010},
  file =        {Exascale_subcommittee_report.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\ZR633C24\\Exascale_subcommittee_report.pdf:application/pdf},
  keywords =    {challenges, exascale systems},
  url =         {http://science.energy.gov/~/media/ascr/ascac/pdf/reports/Exascale_subcommittee_report.pdf},
  urldate =     {2016-05-22}
}

@article{awan_architectural_2016,
	title = {Architectural {Impact} on {Performance} of {In}-memory {Data} {Analytics}: {Apache} {Spark} {Case} {Study}},
	shorttitle = {Architectural {Impact} on {Performance} of {In}-memory {Data} {Analytics}},
	url = {http://arxiv.org/abs/1604.08484},
	urldate = {2016-09-05},
	journal = {arXiv preprint arXiv:1604.08484},
	author = {Awan, Ahsan Javed and Brorsson, Mats and Vlassov, Vladimir and Ayguade, Eduard},
	year = {2016},
	note = {00000},
	file = {Awan et al. - 2016 - Architectural Impact on Performance of In-memory D.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\K3TVEK9M\\Awan et al. - 2016 - Architectural Impact on Performance of In-memory D.pdf:application/pdf}
}

@inproceedings{berral_power-aware_2013,
	title = {Power-{Aware} {Multi}-data {Center} {Management} {Using} {Machine} {Learning}},
	doi = {10.1109/ICPP.2013.102},
	abstract = {The cloud relies upon multi-data center (multi-DC) infrastructures distributed along the world, where people and enterprises pay for resources to offer their web-services to worldwide clients. Intelligent management is required to automate and manage these infrastructures, as the amount of resources and data to manage exceeds the capacities of human operators. Also, it must take into account the cost of running the resources (energy) and the quality of service towards web-services and clients. (De-)consolidation and priming proximity to clients become two main strategies to allocate resources and properly place these web-services in the multi-DC network. Here we present a mathematical model to describe the scheduling problem given web-services and hosts across a multi-DC system, enhancing the decision makers with models for the system behavior obtained using machine learning. After running the system on real DC infrastructures we see that the model drives web-services to the best locations given quality of service, energy consumption, and client proximity, also (de-)consolidating according to the resources required for each web-service given its load.},
	booktitle = {2013 42nd {International} {Conference} on {Parallel} {Processing}},
	author = {Berral, J. L. and Gavaldà, R. and Torres, J.},
	month = oct,
	year = {2013},
	note = {00008},
	keywords = {client proximity, cloud computing, computer centres, consolidation, Energy consumption, intelligent management, learning (artificial intelligence), Machine learning, Mathematical model, Measurement, Monitoring, Multi-DataCenter, multiDC infrastructure, Power-Aware, power aware computing, power-aware multidata center management, Predictive models, priming proximity, quality of service, resource allocation, scheduling, scheduling problem, Time factors, Virtualization, Web services, Web-services},
	pages = {858--867},
	file = {Berral et al_2013_Power-Aware Multi-data Center Management Using Machine Learning.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\9G2XB9KH\\Berral et al_2013_Power-Aware Multi-data Center Management Using Machine Learning.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\KNXR29NJ\\6687426.html:text/html}
}

@InProceedings{BosaghZadeh:2016:MCO:2939672.2939675,
  author =    {Bosagh Zadeh, Reza and Meng, Xiangrui and Ulanov, Alexander and Yavuz, Burak and Pu, Li and Venkataraman, Shivaram and Sparks, Evan and Staple, Aaron and Zaharia, Matei},
  title =     {Matrix Computations and Optimization in Apache Spark},
  booktitle = {Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year =      {2016},
  series =    {KDD '16},
  pages =     {31--38},
  address =   {New York, NY, USA},
  publisher = {ACM},
  acmid =     {2939675},
  doi =       {10.1145/2939672.2939675},
  isbn =      {978-1-4503-4232-2},
  keywords =  {distributed linear algebra, machine learning, matrix computations, mllib, optimization, spark},
  location =  {San Francisco, California, USA},
  numpages =  {8},
  url =       {http://doi.acm.org/10.1145/2939672.2939675}
}

@article{cappello_fault_2009,
	title = {Fault {Tolerance} in {Petascale}/ {Exascale} {Systems}: {Current} {Knowledge}, {Challenges} and {Research} {Opportunities}},
	volume = {23},
	issn = {1094-3420, 1741-2846},
	shorttitle = {Fault {Tolerance} in {Petascale}/ {Exascale} {Systems}},
	url = {http://hpc.sagepub.com/content/23/3/212},
	doi = {10.1177/1094342009106189},
	abstract = {The emergence of petascale systems and the promise of future exascale systems have reinvigorated the community interest in how to manage failures in such systems and ensure that large applications, lasting several hours or tens of hours, are completed successfully. Most of the existing results for several key mechanisms associated with fault tolerance in high-performance computing (HPC) platforms follow the rollback—recovery approach. Over the last decade, these mechanisms have received a lot of attention from the community with different levels of success. Unfortunately, despite their high degree of optimization, existing approaches do not fit well with the challenging evolutions of large-scale systems. There is room and even a need for new approaches. Opportunities may come from different origins: diskless checkpointing, algorithmic-based fault tolerance, proactive operation, speculative execution, software transactional memory, forward recovery, etc. The contributions of this paper are as follows: (1) we summarize and analyze the existing results concerning the failures in large-scale computers and point out the urgent need for drastic improvements or disruptive approaches for fault tolerance in these systems; (2) we sketch most of the known opportunities and analyze their associated limitations; (3) we extract and express the challenges that the HPC community will have to face for addressing the stringent issue of failures in HPC systems.},
	language = {en},
	number = {3},
	urldate = {2016-09-07},
	journal = {Int. J. High Perform. Comput. Appl.},
	author = {Cappello, Franck},
	month = aug,
	year = {2009},
	keywords = {challenges, fault tolerance, knowledge, opportunities, petascale/exascale},
	pages = {212--226},
	file = {Cappello_2009_Fault Tolerance in Petascale- Exascale Systems.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\5A6JUNR5\\Cappello_2009_Fault Tolerance in Petascale- Exascale Systems.pdf:application/pdf;Snapshot:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\VIK759HA\\212.html:text/html}
}

@inproceedings{chintapalli_benchmarking_2016,
	title = {Benchmarking {Streaming} {Computation} {Engines}: {Storm}, {Flink} and {Spark} {Streaming}},
	shorttitle = {Benchmarking {Streaming} {Computation} {Engines}},
	doi = {10.1109/IPDPSW.2016.138},
	abstract = {Streaming data processing has been gaining attention due to its application into a wide range of scenarios. To serve the booming demands of streaming data processing, many computation engines have been developed. However, there is still a lack of real-world benchmarks that would be helpful when choosing the most appropriate platform for serving real-time streaming needs. In order to address this problem, we developed a streaming benchmark for three representative computation engines: Flink, Storm and Spark Streaming. Instead of testing speed-of-light event processing, we construct a full data pipeline using Kafka and Redis in order to more closely mimic the real-world production scenarios. Based on our experiments, we provide a performance comparison of the three data engines in terms of 99th percentile latency and throughput for various configurations.},
	booktitle = {2016 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	author = {Chintapalli, S. and Dagit, D. and Evans, B. and Farivar, R. and Graves, T. and Holderbaugh, M. and Liu, Z. and Nusbaum, K. and Patil, K. and Peng, B. J. and Poulosky, P.},
	month = may,
	year = {2016},
	note = {00001},
	keywords = {benchmark, Benchmark testing, computation engines, data analysis, data engines, data pipeline, Data processing, Engines, Flink, Flink streaming, Kafka, Low Latency, pipeline processing, Pipelines, Redis, Spark, Sparks, Spark streaming, Storm, Storms, Storm streaming, streaming data processing, Streaming processing, Throughput},
	pages = {1789--1792},
	file = {Chintapalli et al_2016_Benchmarking Streaming Computation Engines.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\8KBCCNK4\\Chintapalli et al_2016_Benchmarking Streaming Computation Engines.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\7MPHUMD3\\7530084.html:text/html}
}

@inproceedings{cochran_consistent_2010,
	address = {New York, NY, USA},
	series = {{DAC} '10},
	title = {Consistent {Runtime} {Thermal} {Prediction} and {Control} {Through} {Workload} {Phase} {Detection}},
	isbn = {978-1-4503-0002-5},
	url = {http://doi.acm.org/10.1145/1837274.1837292},
	doi = {10.1145/1837274.1837292},
	abstract = {Elevated temperatures impact the performance, power consumption, and reliability of processors, which rely on integrated thermal sensors to measure runtime thermal behavior. These thermal measurements are typically inputs to a dynamic thermal management system that controls the operating parameters of the processor and cooling system. The ability to predict future thermal behavior allows a thermal management system to optimize a processor's operation so as to prevent the on-set of high temperatures. In this paper we propose a new thermal prediction method that leads to consistent results between the thermal models used in prediction and observed thermal sensor measurements, and is capable of accurately predicting temperature behavior with heterogenous workload assignment on a multicore platform. We devise an off-line analysis algorithm that learns a set of thermal models as a function of operating frequency and globally defined workload phases. We incorporate these thermal models into a dynamic voltage and frequency scaling (DVFS) technique that limits the maximum temperature during runtime. We demonstrate the effectiveness of our proposed system in predicting the thermal behavior of a real quad-core processor in response to different workloads. In comparison to a reactive thermal management technique, our predictive method dramatically reduces the number of thermal violations, the magnitude of thermal cycles, and workload runtimes.},
	urldate = {2016-09-04},
	booktitle = {Proceedings of the 47th {Design} {Automation} {Conference}},
	publisher = {ACM},
	author = {Cochran, Ryan and Reda, Sherief},
	year = {2010},
	note = {00059},
	keywords = {DVFS, multicore systems, proactive control, thermal prediction, thermal sensing, workload phase},
	pages = {62--67},
	file = {Cochran_Reda_2010_Consistent Runtime Thermal Prediction and Control Through Workload Phase.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\9GE36SHN\\Cochran_Reda_2010_Consistent Runtime Thermal Prediction and Control Through Workload Phase.pdf:application/pdf}
}
@inproceedings{gutierrez_neural_2015,
	title = {Neural network methods for fast and portable prediction of {CPU} power consumption},
	doi = {10.1109/IGCC.2015.7393702},
	abstract = {The need for energy efficient computing has established performance-per-watt as a first-class metric for evaluating HPC applications. Consequently, optimizations that target HPC systems and data centers are required to dynamically monitor system power consumption in order to be effective. Although newer architectures are making power sensors available on the chip, the general state of power measurement tools across different architectures remains deficient. This paper describes a neural-network based model for fine-grain, accurate and low-cost power estimation. The main novelty of the proposed approach is its portability. The methodology can be adopted to predict power consumption not only on a range of current processors but future architectures as well. This portability is achieved by taking advantage of performance monitoring units (PMU) available on current systems and applying a carefully constructed sequence of feature selection techniques. We evaluate our models along several dimensions on multiple platforms. The experimental results show that the constructed models are able to predict power consumption with high accuracy at a low overhead. The results also provide key insight as to the number of features necessary to achieve reasonable prediction accuracy.},
	booktitle = {Green {Computing} {Conference} and {Sustainable} {Computing} {Conference} ({IGSC}), 2015 {Sixth} {International}},
	author = {Gutierrez, M. and Rahman, S. and Tamir, D. and Qasem, A.},
	month = dec,
	year = {2015},
	note = {00000},
	keywords = {CPU power consumption, energy efficient computing, feature selection, feature selection techniques, HPC systems, Lead, neural nets, neural network methods, Parallel processing, performance monitoring units, PMU, power aware computing},
	pages = {1--4},
	file = {Gutierrez et al_2015_Neural network methods for fast and portable prediction of CPU power consumption.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\TAM6SDHM\\Gutierrez et al_2015_Neural network methods for fast and portable prediction of CPU power consumption.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\U6RA723M\\abs_all.html:text/html}
}

@inproceedings{harton_towards_2015,
	title = {Towards {Power} {Consumption} {Modeling} for {Servers} at {Scale}},
	doi = {10.1109/UCC.2015.50},
	abstract = {As of 2010 data centers use 1.5\% of global electricity production and this is expected to keep growing [1]. There is a need for a near real-time power consumption modeling/monitoring system that could be used at scale within a Software Defined Data Center (SDDC). The power consumption models and information they provide can then be used to make better decisions for data center orchestration, e.g., whether to migrate virtual machines to reduce power consumption. We propose a scalable system that would 1) create initial power consumption models, as needed, for data center components, and 2) could be continually refined while the components are in use. The models will be used for the near real-time monitoring of power consumption, as well as predicting power consumption before and after potential orchestration decisions. The first step towards this goal of whole data center power modeling and prediction is to be able to predict the power consumption of one server effectively, based on high level utilization statistics from that server. In this paper we present a novel method for modeling whole system power consumption for a server, under varying random levels of CPU utilization, with a scalable random forest based model, that utilizes statistics available at the data center management level.},
	booktitle = {2015 {IEEE}/{ACM} 8th {International} {Conference} on {Utility} and {Cloud} {Computing} ({UCC})},
	author = {Harton, T. W. and Walker, C. and O'Sullivan, M.},
	month = dec,
	year = {2015},
	note = {00000},
	keywords = {2010 data centers, Benchmark testing, Computational modeling, computer centres, CPU utilization, data center components, data center management level, data center orchestration, Data models, energy-efficiency, global electricity production, Load modeling, Monitoring, power aware computing, power consumption estimation, power consumption modeling/monitoring system, Power consumption models, Power demand, processor power consumption, random forests, reduce power consumption models, scalable random forest based model, SDDC, server power consumption, Servers, software defined data center, virtual machines, whole data center power modeling},
	pages = {315--321},
	file = {Harton et al_2015_Towards Power Consumption Modeling for Servers at Scale.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\VR7EMWD5\\Harton et al_2015_Towards Power Consumption Modeling for Servers at Scale.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\UBE72CQC\\7431425.html:text/html}
}

@article{meng_mllib:_2016,
	title = {Mllib: {Machine} learning in apache spark},
	volume = {17},
	shorttitle = {Mllib},
	url = {http://www.jmlr.org/papers/volume17/15-237/15-237.pdf},
	number = {34},
	urldate = {2016-09-16},
	journal = {JMLR},
	author = {Meng, Xiangrui and Bradley, Joseph and Yuvaz, B. and Sparks, Evan and Venkataraman, Shivaram and Liu, Davies and Freeman, Jeremy and Tsai, D. and Amde, Manish and Owen, Sean and {others}},
	year = {2016},
	note = {00094},
	pages = {1--7},
	file = {15-237.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\CXH6F3V4\\15-237.pdf:application/pdf}
}

@inproceedings{nishtala_methodology_2015,
	title = {A {Methodology} to {Build} {Models} and {Predict} {Performance}-{Power} in {CMPs}},
	doi = {10.1109/ICPPW.2015.29},
	abstract = {Data centers have time-varying traffic and a wide range of demands in performance-power for the workloads. Understanding the trade-offs between performance and power for these varying types of demands given the time-variations, helps administrators to control the total power consumption and also facilitate for enhancing Quality of Service (QoS) levels. In this paper, we provide a methodology for administrators to predict performance and power using performance monitoring counters at all processor frequency states (P-States) and across a wide range of processor sleep intervals (Cl-States). This methodology will allow administrators to make quick decisions. Specifically, we build models and validate them using SPECcpu2006 benchmarks on an Intel Sandy Bridge processor with 4 cores, 9 P-States and 50 Cl-States resulting in a total of 40 billion configurations when running 4 threads in parallel. The modeling technique provided is fast (under 60,000 cycles), and requires a small period of time to build (less than 10 hours). The modeling technique predicts with a high accuracy and the results obtained show an average error in prediction of 2.25\% and 13.99\% (with a 95\% confidence interval) for power and performance, respectively. Moreover, we provide an insight into the usability of these models in multi-core architectures by predicting performance and power for 3 different types of workloads under 9 different types of QoS constraints. The results obtained show an error in prediction of 9.02\% and 7.45\% (with a 95\% confidence interval) for the total energy and performance delay respectively in multi-core architectures.},
	booktitle = {2015 44th {International} {Conference} on {Parallel} {Processing} {Workshops} ({ICPPW})},
	author = {Nishtala, R. and Tallada, M. G. and Martorell, X.},
	month = sep,
	year = {2015},
	note = {00000},
	keywords = {Benchmark testing, Cl-States, CMP, Computational modeling, computer centres, Data Center, Hardware, High performance computing, HPC, Mathematical model, Microarchitecture, multicore architecture, multiprocessing systems, Parallel processing, performance-power prediction, Predictive models, processor frequency states, processor sleep intervals, P-States, QoS, quality of service},
	pages = {193--202},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\87XCTMV5\\abs_all.html:text/html;Nishtala et al_2015_A Methodology to Build Models and Predict Performance-Power in CMPs.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\SWSKEEZF\\Nishtala et al_2015_A Methodology to Build Models and Predict Performance-Power in CMPs.pdf:application/pdf}
}

@inproceedings{sarood_maximizing_2014,
	title = {Maximizing throughput of overprovisioned {HPC} data centers under a strict power budget},
	url = {http://dl.acm.org/citation.cfm?id=2683682},
	urldate = {2016-09-12},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE Press},
	author = {Sarood, Osman and others},
	year = {2014},
	keywords = {HPC, power capping},
	pages = {807--818},
	file = {paper.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\J3RR2QHB\\paper.pdf:application/pdf}
}

@article{tsafack_chetsa_exploiting_2014,
	series = {Special {Section}: {Intelligent} {Big} {Data} {ProcessingSpecial} {Section}: {Behavior} {Data} {Security} {Issues} in {Network} {Information} {PropagationSpecial} {Section}: {Energy}-efficiency in {Large} {Distributed} {Computing} {ArchitecturesSpecial} {Section}: {eScience} {Infrastructure} and {Applications}},
	title = {Exploiting performance counters to predict and improve energy performance of {HPC} systems},
	volume = {36},
	issn = {0167-739X},
	url = {http://www.sciencedirect.com/science/article/pii/S0167739X13001556},
	doi = {10.1016/j.future.2013.07.010},
	abstract = {Hardware monitoring through performance counters is available on almost all modern processors. Although these counters are originally designed for performance tuning, they have also been used for evaluating power consumption. We propose two approaches for modelling and understanding the behaviour of high performance computing (HPC) systems relying on hardware monitoring counters. We evaluate the effectiveness of our system modelling approach considering both optimizing the energy usage of HPC systems and predicting HPC applications’ energy consumption as target objectives. Although hardware monitoring counters are used for modelling the system, other methods–including partial phase recognition and cross platform energy prediction–are used for energy optimization and prediction. Experimental results for energy prediction demonstrate that we can accurately predict the peak energy consumption of an application on a target platform; whereas, results for energy optimization indicate that with no a priori knowledge of workloads sharing the platform we can save up to 24\% of the overall HPC system’s energy consumption under benchmarks and real-life workloads.},
	urldate = {2016-09-04},
	journal = {Future Generation Computer Systems},
	author = {Tsafack Chetsa, G. L. and Lefèvre, L. and Pierson, J. M. and Stolf, P. and Da Costa, G.},
	month = jul,
	year = {2014},
	note = {00014},
	keywords = {Energy performance, Green IT, Hardware performance counters, High performance computing, power consumption},
	pages = {287--298},
	file = {ScienceDirect Snapshot:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\4PQA5TH6\\S0167739X13001556.html:text/html;Tsafack Chetsa et al_2014_Exploiting performance counters to predict and improve energy performance of.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\4TIXIFWX\\Tsafack Chetsa et al_2014_Exploiting performance counters to predict and improve energy performance of.pdf:application/pdf}
}

@inproceedings{wang_flexible_2011,
	title = {A flexible architecture integrating monitoring and analytics for managing large-scale data centers},
	url = {http://dl.acm.org/citation.cfm?id=1998605},
	urldate = {2016-09-08},
	booktitle = {Proceedings of the 8th {ACM} international conference on {Autonomic} computing},
	publisher = {ACM},
	author = {Wang, Chengwei and Schwan, Karsten and Talwar, Vanish and Eisenhauer, Greg and Hu, Liting and Wolf, Matthew},
	year = {2011},
	note = {00056},
	keywords = {analytics, Data Center, Monitoring},
	pages = {141--150},
	file = {d82031524e14de70b0e9e5a824c72742d3d7.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\PHXRPUGS\\d82031524e14de70b0e9e5a824c72742d3d7.pdf:application/pdf}
}

@inproceedings{zaharia_resilient_2012,
	title = {Resilient distributed datasets: {A} fault-tolerant abstraction for in-memory cluster computing},
	shorttitle = {Resilient distributed datasets},
	url = {http://dl.acm.org/citation.cfm?id=2228301},
	urldate = {2016-09-16},
	booktitle = {Proceedings of the 9th {USENIX} conference on {Networked} {Systems} {Design} and {Implementation}},
	publisher = {USENIX Association},
	author = {Zaharia, Matei and others},
	year = {2012},
	pages = {2--2},
	file = {nsdi_spark.pdf:C\:\\Users\\Francesco\\Dropbox\\Zotero\\storage\\MQ36VHHG\\nsdi_spark.pdf:application/pdf}
}


@INPROCEEDINGS{ALINA15, 
author={A. Srbu and O. Babaoglu}, 
booktitle={Cloud and Autonomic Computing (ICCAC), 2015 International Conference on}, 
title={Towards Data-Driven Autonomics in Data Centers}, 
year={2015}, 
pages={45-56}, 
keywords={SQL;computer centres;fault tolerant computing;pattern classification;query processing;random processes;BigQuery;Google Cloud suite;autonomic manager;big data SQL platform;central component;classification analysis;computer system;data center;data log;data stream;data-driven autonomics;data-science study;data-science tool;holistic predictive model;human operator;low-level operation;node failure;predictive computational model;public Google dataset;random forest classifier;Correlation;Google;Predictive models;Radio frequency;Testing;Training;Training data;BigQuery;Data science;Google cluster trace;ensemble classifier;failure prediction;log data analysis;machine learning classification;predictive analytics;random forest}, 
doi={10.1109/ICCAC.2015.19}, 
month={Sept},}

@article{POWERDAM,
title = "Monitoring Power Data: A first step towards a unified energy efficiency evaluation toolset for {HPC} data centers ",
journal = "Environmental Modelling \& Software ",
volume = "56",
number = "",
pages = "13 - 26",
year = "2014",
issn = "1364-8152",
doi = "http://dx.doi.org/10.1016/j.envsoft.2013.11.011",
url = "http://www.sciencedirect.com/science/article/pii/S1364815213002934",
author = "Hayk Shoukourian and others",
keywords = "Energy consumption",
keywords = "PowerDAM",
keywords = "Energy-to-Solution (EtS)",
keywords = "Energy measurement",
keywords = "Energy efficiency toolset",
keywords = "HPC data center "
}

@Inbook{ISC16,
author="Borghesi, Andrea
and Bartolini, Andrea
and Lombardi, Michele
and Milano, Michela
and Benini, Luca",
title="Predictive Modeling for Job Power Consumption in HPC Systems",
bookTitle="High Performance Computing: 31st International Conference, ISC High Performance 2016, Frankfurt, Germany, June 19-23, 2016, Proceedings",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="181--199",
isbn="978-3-319-41321-1",
doi="10.1007/978-3-319-41321-1_10",
url="http://dx.doi.org/10.1007/978-3-319-41321-1_10"
}

@article{GANGLIA,
title = "The ganglia distributed monitoring system: design, implementation, and experience ",
journal = "Parallel Computing ",
volume = "30",
number = "7",
pages = "817 - 840",
year = "2004",
note = "",
issn = "0167-8191",
doi = "http://dx.doi.org/10.1016/j.parco.2004.04.001",
url = "http://www.sciencedirect.com/science/article/pii/S0167819104000535",
author = "Matthew L Massie and Brent N Chun and David E Culler",
keywords = "Monitoring",
keywords = "Clusters",
keywords = "Distributed systems "
}

@article{PMU,
title = "Energy-aware performance analysis methodologies for {HPC} architectures: An exploratory study ",
journal = "Journal of Network and Computer Applications ",
volume = "35",
number = "6",
pages = "1709 - 1719",
year = "2012",
note = "",
issn = "1084-8045",
doi = "http://dx.doi.org/10.1016/j.jnca.2012.08.003",
url = "http://www.sciencedirect.com/science/article/pii/S1084804512001798",
author = "Shajulin Benedict",
keywords = "HPC",
keywords = "Performance analysis",
keywords = "Tools",
keywords = "Energy monitoring "
}

@INPROCEEDINGS{MQTT, 
author={S. Lee and H. Kim and D. k. Hong and H. Ju}, 
booktitle={The International Conference on Information Networking 2013 (ICOIN)}, 
title={Correlation analysis of MQTT loss and delay according to QoS level}, 
year={2013}, 
pages={714-717}, 
keywords={client-server systems;message authentication;network servers;protocols;quality of service;telecommunication network reliability;IBM;MQTT loss;MQTT message transmission process;QoS level;broker server;correlation analysis;end-to-end delays;message loss;open protocol;real wired-wireless publish client;reliability;subscribe client;Correlation;Delays;Payloads;Protocols;Quality of service;Servers;Wireless networks;Broker server;MQTT;Publish;Subscribe}, 
doi={10.1109/ICOIN.2013.6496715}, 
ISSN={1550-445X}, 
month={Jan},}

@inproceedings{MSR,
  title={Whitelisting MSRs with msr-safe},
  author={Shoga, Kathleen and Rountree, Barry and Schulz, Martin and Shafer, Jeff},
  booktitle={3rd Workshop on Exascale Systems Programming Tools, in conjunction with SC14},
  year={2014}
}

@INPROCEEDINGS{RTDA,
author={S. J. Morshed and J. Rana and M. Milrad},
booktitle={2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
title={Open Source Initiatives and Frameworks Addressing Distributed Real-Time Data Analytics},
year={2016},
pages={1481-1484},
keywords={Big Data;data analysis;distributed processing;Apache Flink;Apache Samza;Apache Spark;Apache Storm;RTDA frameworks;big data problems;digital services;distributed real-time data analytics;open source initiatives;Big data;Data analysis;Distributed databases;Real-time systems;Sparks;Storms;Real-time;big data;data analytics;data analytics framework;distributed real-time data analysis;streaming data},
doi={10.1109/IPDPSW.2016.152},
month={May},}


@inproceedings{CONF14,
  title={Energy-aware cooling for hot-water cooled supercomputers},
  author={Conficoni, Christian and Bartolini, Andrea and Tilli, Andrea and Tecchiolli, Giampietro and Benini, Luca},
  booktitle={Proceedings of the 2015 Design, Automation \& Test in Europe Conference \& Exhibition},
  pages={1353--1358},
  year={2015},
  organization={EDA Consortium}
}

@misc{light2013mosquitto,
  title={Mosquitto-an open source mqtt v3. 1 broker},
  author={Light, R},
  year={2013}
}

%%%%%%%%%%%%
% Examon Web
%%%%%%%%%%%%
@article{mikowski2013single,
  title={Single page web applications},
  author={Mikowski, Michael S and Powell, Josh C},
  journal={B and W},
  year={2013}
}

@book{rai2013socket,
  title={Socket. IO Real-time Web Application Development},
  author={Rai, Rohit},
  year={2013},
  publisher={Packt Publishing Ltd}
}

@inproceedings{yasin2014top,
  title={A top-down method for performance analysis and counters architecture},
  author={Yasin, Ahmad},
  booktitle={Performance Analysis of Systems and Software (ISPASS), 2014 IEEE International Symposium on},
  pages={35--44},
  year={2014},
  organization={IEEE}
}

@INPROCEEDINGS{examon, 
author={F. Beneventi and A. Bartolini and C. Cavazzoni and L. Benini}, 
booktitle={Design, Automation Test in Europe Conference Exhibition (DATE), 2017}, 
title={Continuous learning of HPC infrastructure models using big data analytics and in-memory processing tools}, 
year={2017}, 
pages={1038-1043}, 
keywords={Computational modeling;Data models;Mathematical model;Monitoring;Protocols;Real-time systems;Tools}, 
doi={10.23919/DATE.2017.7927143}, 
month={March},}

@misc{IPMI,
  title={IPMI--A Gentle Introduction with OpenIPMI},
  author={Minyard, Corey}
}

@article{VTUNE,
  title={VTune performance analyzer essentials},
  author={Reinders, James},
  journal={Intel Press},
  year={2005}
}

@misc{PCM,
	title 	= {Intelnitor/.PBS Professional\textregistered Performance Counter Monitor. },
	url = {http://software.intel.com/en-us/ articles/intel-performance-counter-monitor/.}
}

@inproceedings{PERF,
  title={The new linux’perf’tools},
  author={de Melo, Arnaldo Carvalho},
  booktitle={Slides from Linux Kongress},
  volume={18},
  year={2010}
}

@inproceedings{PAPI,
  title={Using PAPI for hardware performance monitoring on Linux systems},
  author={Dongarra, Jack and London, Kevin and Moore, Shirley and Mucci, Phil and Terpstra, Dan},
  booktitle={Conference on Linux Clusters: The HPC Revolution},
  volume={5},
  year={2001},
  organization={Linux Clusters Institute}
}

@article{SPARK,
  title={Apache Spark: A unified engine for big data processing},
  author={Zaharia, Matei and Xin, Reynold S and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J and others},
  journal={Communications of the ACM},
  volume={59},
  number={11},
  pages={56--65},
  year={2016},
  publisher={ACM}
}

@article{CASSANDRA,
  title={Cassandra: a decentralized structured storage system},
  author={Lakshman, Avinash and Malik, Prashant},
  journal={ACM SIGOPS Operating Systems Review},
  volume={44},
  number={2},
  pages={35--40},
  year={2010},
  publisher={ACM}
}

@misc{Grafana,
  title={Grafana, The Leading Graph And Dashboard Builder For Visualizing Time Series Metrics},
  author={{\"O}degaard, T},
  year={2016}
}

@article{locke2010mqtt,
  title={Mq telemetry transport (mqtt) v3. 1 protocol specification},
  author={Locke, Dave},
  journal={IBM developerWorks Technical Library},
  year={2010}
}

@misc{KAIROS,
    title = {{KairosDB}},
    howpublished = {\url{https://kairosdb.github.io}},
    note = {Accessed: 2017-11-25}
}

@book{deeplearning,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@book{elements,
  title={Elements of artificial neural networks},
  author={Mehrotra, Kishan and Mohan, Chilukuri K and Ranka, Sanjay},
  year={1997},
  publisher={MIT press}
}

@article{scipy,
  title={$\{$SciPy$\}$: open source scientific tools for $\{$Python$\}$},
  author={Jones, Eric and Oliphant, Travis and Peterson, Pearu},
  year={2014}
}
